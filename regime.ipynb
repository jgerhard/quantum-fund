{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime Predictor\n",
    "\n",
    "Rewriting the PCA-based regime projection from the ML and RL in Finance course by Igor Halperin. Theory is based on \n",
    "*Principal Components as a Measure of Systemic Risk* by *Mark Kritzman, Yuanzhen Li, Sebastien Page, and Roberto Rigobon*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 13,8\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset:  daily prices of for market representative such as S&P 500, MSCI World, etc. ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./individual_stocks/' # use your path\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "li = []\n",
    "cols = []\n",
    "for filename in all_files[:]:\n",
    "    df = pd.read_csv(filename, index_col=0, header=0, date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'))\n",
    "    df = df.Close\n",
    "    cols.append(filename.partition(\"_\")[-1].partition(\"_\")[0].strip(\"stocks/\"))\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=1, ignore_index=True)\n",
    "frame = frame.rename(columns=dict(zip(range(len(all_files)), cols)))\n",
    "asset_prices = frame.interpolate().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "print('Asset prices shape', asset_prices.shape)\n",
    "display(asset_prices.iloc[:, :n_stocks_show].head(-3))\n",
    "asset_prices.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate daily log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns = np.log(asset_prices) - np.log(asset_prices.shift(1))\n",
    "# asset_returns = asset_prices.pct_change(periods=1) #log(1+r) approx r\n",
    "asset_returns = asset_returns.iloc[1:, :]\n",
    "asset_returns.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_returns(df):\n",
    "    \"\"\"\n",
    "    Normalize, i.e. center and divide by standard deviation raw asset returns data\n",
    "\n",
    "    Arguments:\n",
    "    r_df -- a pandas.DataFrame of asset returns\n",
    "\n",
    "    Return:\n",
    "    normed_df -- normalized returns\n",
    "    \"\"\"\n",
    "    return (df - df.mean(axis=0))/df.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_r = normalize_returns(asset_returns)\n",
    "display(normed_r.iloc[:, :n_stocks_show].head(-3))\n",
    "#easier than standardscaler from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the Absorption Ratio(AR) by taking a moving window of the returns for which we compute the covariance matrix and do a PCA. The data is based on step_size (business daily, ...) Then we look, how much variation of the complete market is determined already by 1/5 of the constitutents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absorption_ratio(explained_variance, n_components):\n",
    "    \"\"\"\n",
    "    Calculate absorption ratio via PCA.\n",
    "    \n",
    "    Arguments:\n",
    "    explained_variance -- 1D np.array of explained variance by each pricincipal component, in descending order\n",
    "    n_components -- an integer, a number of principal components to compute absorption ratio\n",
    "    Return:\n",
    "    ar -- absorption ratio\n",
    "    \"\"\"\n",
    "    ar = np.sum(explained_variance[:n_components]) / np.sum(explained_variance)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "\n",
    "lookback_window = 252 * 2   # in (days)\n",
    "num_assets = len(stock_tickers)\n",
    "step_size = 1          # days : 5 - weekly, 21 - monthly, 63 - quarterly\n",
    "\n",
    "# fix 20% of principal components for absorption ratio calculation. How much variance do they explain?\n",
    "absorb_comp = int((1 / 5) * num_assets)  \n",
    "\n",
    "print('Lookback window = %d' % lookback_window)\n",
    "print('Step size = %d' % step_size)\n",
    "print('Number of stocks = %d' % num_assets)\n",
    "print('Number of principal components = %d' % absorb_comp)\n",
    "\n",
    "# indexes date on which to compute PCA\n",
    "days_offset = 4 * 252\n",
    "num_days = 12 * 252 + days_offset\n",
    "pca_ts_index = normed_r.index[list(range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size))]\n",
    "\n",
    "# allocate array for storing absorption ratio\n",
    "absorp_ratio = np.array([np.nan]*len(pca_ts_index))\n",
    "\n",
    "assert 'SPX' not in normed_r.iloc[:lookback_window, :-1].columns.values, \"By accident included SPX index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "ik = 0\n",
    "for ix in range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size):\n",
    "    ret_frame = normed_r.iloc[(ix - lookback_window) : ix, :-1]  # fixed window\n",
    "    cov_mat = ret_frame.cov()\n",
    "    \n",
    "    if ik == 0 or ik % 21 == 0: # to speed up convergence compute just once a month?\n",
    "        pca = sklearn.decomposition.PCA() #restrict to n_components=absorb_comp and rely on the ratio\n",
    "        pca.fit(cov_mat)\n",
    "        #absorp_ratio[ik]= pca.explained_variance_ratio_.sum()\n",
    "        absorp_ratio[ik] = absorption_ratio(pca.explained_variance_, absorb_comp) \n",
    "    else:\n",
    "        absorp_ratio[ik] = absorp_ratio[ik-1]       \n",
    "    ik += 1\n",
    "\n",
    "ts_absorb_ratio = pd.Series(absorp_ratio, index=pca_ts_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how much variance do the fix 20% of principal components explain?\n",
    "sns.lineplot(data=ts_absorb_ratio, linewidth=3)\n",
    "plt.title('Absorption Ratio via PCA')\n",
    "#plt.savefig(\"Absorption_Ratio_SPX.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving a trading strategy from the absorption ratio according to the Kritzman paper\n",
    "Having computed daily (this means the step size is 1) Absorption Ratio times series, we further follow M. Kritzman to make use of AR to define yet another measure: AR Delta. In particular:\n",
    "$$ AR\\delta = \\frac{AR_{15d} - AR_{1y}}{ AR\\sigma_{1y}}$$\n",
    "We use  $AR\\delta$ to build simple portfolio trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following Kritzman and computing AR_delta = (15d_AR -1yr_AR) / sigma_AR\n",
    "ts_ar = ts_absorb_ratio\n",
    "ar_mean_1yr = ts_ar.rolling(252).mean()\n",
    "ar_mean_15d = ts_ar.rolling(15).mean()\n",
    "ar_sd_1yr = ts_ar.rolling(252).std()\n",
    "ar_delta = (ar_mean_15d - ar_mean_1yr) / ar_sd_1yr    # standardized shift in absorption ratio\n",
    "\n",
    "df_plot = pd.DataFrame({'AR_delta': ar_delta.values, 'AR_1yr': ar_mean_1yr.values, 'AR_15d': ar_mean_15d.values}, \n",
    "                       index=ts_ar.index)\n",
    "df_plot = df_plot.dropna()\n",
    "if df_plot.shape[0] > 0:\n",
    "    sns.lineplot(data=df_plot, linewidth=3)\n",
    "    plt.title(\"Absorption Ratio Delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (AR Delta Trading Strategy)\n",
    "\n",
    "The AR Delta trading strategy forms a portfolio of EQ and FI, following these simple rules:\n",
    "\n",
    "* __$ -1\\sigma < AR < +1\\sigma $__\t 50 / 50 weights for EQ / FI\n",
    "* __$ AR > +1\\sigma $__\t             0 / 100 weights for EQ / FI\n",
    "* __$ AR < -1\\sigma $__\t             100 / 0 weights for EQ / FI\n",
    "\n",
    "Here we compute AR Delta strategy weights using data from the same data set. As expected, the average number of trades per year is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(ar_delta):\n",
    "    '''\n",
    "    Calculate EQ / FI portfolio weights based on Absorption Ratio delta\n",
    "    Arguments:\n",
    "    ar_delta -- Absorption Ratio delta\n",
    "    \n",
    "    Return: \n",
    "        wgts -- a vector of portfolio weights\n",
    "    '''\n",
    "    if ar_delta > 1: return [0.0,1.0]\n",
    "    if ar_delta < -1: return [1.0, 0.0]\n",
    "    return [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_delta_data = ar_delta[251:]\n",
    "\n",
    "rebal_dates = np.zeros(len(ar_delta_data))\n",
    "wgts = pd.DataFrame(data=np.zeros((len(ar_delta_data.index), 2)), index=ar_delta_data.index, columns=('EQ', 'FI'))\n",
    "\n",
    "prtf_wgts = get_weight(ar_delta_data.values[0])\n",
    "wgts.iloc[0, :] = prtf_wgts\n",
    "for ix in range(1, len(ar_delta_data)):\n",
    "    prtf_wgts = get_weight(ar_delta_data.values[ix])\n",
    "    wgts.iloc[ix, :] = prtf_wgts\n",
    "    if wgts.iloc[ix-1, :][0] != prtf_wgts[0]:\n",
    "        rebal_dates[ix] = 1\n",
    "\n",
    "ts_rebal_dates = pd.Series(rebal_dates, index=ar_delta_data.index)\n",
    "ts_trades_per_year = ts_rebal_dates.groupby([ts_rebal_dates.index.year]).sum()\n",
    "print('Average number of trades per year %.2f' % ts_trades_per_year.mean())\n",
    "display(wgts.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance of backtested strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_strategy(strat_wgts, asset_returns, periods_per_year = 252):\n",
    "    '''\n",
    "    Calculate portfolio returns and return portfolio strategy performance\n",
    "    Arguments:\n",
    "    \n",
    "    strat_wgts -- pandas.DataFrame of weights of the assets\n",
    "    asset_returns -- pandas.DataFrame of asset returns\n",
    "    periods_per_year -- number of return observations per year\n",
    "    \n",
    "    Return: \n",
    "        (ann_ret, ann_vol, sharpe) -- a tuple of (annualized return, annualized volatility, sharpe ratio)\n",
    "    '''\n",
    "    \n",
    "    together = pd.merge(strat_wgts, asset_returns, left_index=True, right_index=True)\n",
    "    together[\"Returns\"] = together[\"EQ\"]*together[\"EQ_RETURN\"]+together[\"FI\"]*together[\"FI_RETURN\"]\n",
    "    annualized_return = np.prod(together.Returns.values + 1)**(periods_per_year/len(together.Returns))-1\n",
    "    annualized_volatility = together.Returns.std() * np.sqrt(periods_per_year)\n",
    "    return annualized_return, annualized_volatility, annualized_return/annualized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ret, ann_vol, sharpe = backtest_strategy(wgts, etf_r)\n",
    "print('Absorption Ratio strategy:', ann_ret, ann_vol, sharpe)\n",
    "eq_wgts = wgts.copy()\n",
    "eq_wgts.iloc[:, ] = 0.5\n",
    "ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt = backtest_strategy(eq_wgts, etf_r)\n",
    "print('Equally weighted:', ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "machine-learning-in-finance"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
